{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working Station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                      Accenture plc\n",
       "1                                    CGI Group, Inc.\n",
       "2         Cognizant Technology Solutions Corporation\n",
       "3                                    Infosys Limited\n",
       "4                                                IBM\n",
       "5                                      Wipro Limited\n",
       "6            Booz Allen Hamilton Holding Corporation\n",
       "7                             DXC Technology Company\n",
       "8                                 EPAM Systems, Inc.\n",
       "9                                      Gartner, Inc.\n",
       "10                             Leidos Holdings, Inc.\n",
       "11    Science Applications International Corporation\n",
       "12                           ICF International, Inc.\n",
       "13                         Insight Enterprises, Inc.\n",
       "14                 ManTech International Corporation\n",
       "15                                  Perficient, Inc.\n",
       "16                                Unisys Corporation\n",
       "17                               Virtusa Corporation\n",
       "18                        Edgewater Technology, Inc.\n",
       "19                            RCM Technologies, Inc.\n",
       "Name: Company, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the names of the companies\n",
    "alldat = pd.read_csv('Publicly_Traded_IT_Consulting_Firms.csv', delimiter=',') # 파일명을 바꾸세요\n",
    "names = alldat['Company']\n",
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_company_token(company):\n",
    "    '''\n",
    "    Every company's Glassdoor review page url includes a randomly-assigned token (e.g., Booz Allen Hamilton: E2735)\n",
    "    This is an automated selenium program that collects the token for each companies\n",
    "    '''\n",
    "    \n",
    "    # Set up the webdriver (make sure that the driver is saved in the same path as the Jupyter notebook file)\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--ignore-certificate-errors')\n",
    "    options.add_argument(\"--test-type\")\n",
    "    options.binary_location = \"/usr/bin/chromium\"\n",
    "    driver = webdriver.Chrome() # paranthesis is empty since the driver is in the same path\n",
    "    driver.get('https://www.google.com') # it is faster to google than to search for the company on Glassdoor (crappy site)\n",
    "    \n",
    "    # Iterate through the list of company names\n",
    "    search_text = 'glassdoor ' + company + ' Reviews'\n",
    "    search = driver.find_element_by_name('q')\n",
    "    search.send_keys(search_text)\n",
    "    search.send_keys(Keys.RETURN) # hits return after you enter the search text\n",
    "    time.sleep(1)\n",
    "    links = driver.find_elements_by_partial_link_text('https://www.glassdoor.com/Reviews/')\n",
    "    full_link = links[0].get_attribute('href')\n",
    "    company_token = re.search('(?<=Reviews/)([\\w-]*)(?=.htm)',full_link)[1] # the token is between 'Reviews/' and '.htm'\n",
    "    driver.quit()\n",
    "    \n",
    "    return company_token\n",
    "\n",
    "\n",
    "def get_overall_stats(webpage_text):\n",
    "    '''\n",
    "    Overall summary of all the reviews posted for the company as of (today)\n",
    "    '''\n",
    "    overall_str = re.search('{\"overallRating\":[^;]*\"EmployerRatings\"}', webpage_text).group(0)\n",
    "    overall_dict = json.loads(overall_str)\n",
    "    \n",
    "    return overall_dict\n",
    "\n",
    "\n",
    "def get_individual_reviews(webpage_text):\n",
    "    '''\n",
    "    Each review posted for the company as of (today)\n",
    "    '''\n",
    "    review_lst = re.findall('{\"reviewId\":.*?(?:.reviews.\\d\":|\"ROOT_QUERY\")', apollo) # .*? means \"any\" ; ?: will prevent interference\n",
    "    review_lst = [review[:-12]+'}' for review in review_lst] # HARDCODING. need a better solution..\n",
    "    review_dct_lst = []\n",
    "\n",
    "    for review in review_lst:\n",
    "        review_first_half = review.split(',\"links\":')[0]+'}'\n",
    "        review_second_half = review.split(',\"links\":')[1]\n",
    "\n",
    "        try:\n",
    "            temp_dct = json.loads(review_first_half)\n",
    "        except ValueError:  # Includes simplejson.decoder.JSONDecodeError. Avoids JSONDecode Error: Invalid \\escape\n",
    "            if '\\\\<' in review:\n",
    "                temp_dct = json.loads(review_first_half.replace('\\\\<', '<')) # HARDCODING. need a better solution..\n",
    "            elif '\\\\>' in review:\n",
    "                temp_dct = json.loads(review_first_half.replace('\\\\>', '>')) # HARDCODING. need a better solution..\n",
    "\n",
    "        # Look for the reviewer's job title\n",
    "        reviewer_job = re.findall('(?<=,\"text\":\")(.*?)(?=\",\"__typename\":\"JobTitle\")', review_second_half)\n",
    "        try:\n",
    "            temp_dct['jobTitle'] = reviewer_job[0]\n",
    "        except:\n",
    "            temp_dct['jobTitle'] = reviewer_job\n",
    "\n",
    "        # Look for the reviewer's location\n",
    "        reviewer_loc = re.findall('(?<=,\"type\":\")(.*?)(?=\",\"__typename\":\"Location)', review_second_half)\n",
    "        if reviewer_loc == []:\n",
    "            temp_dct['Location'] = []\n",
    "        else:\n",
    "            loc_information = reviewer_loc[0].split('\"')\n",
    "            temp_dct['Location'] = {'locationType': loc_information[0], 'locationName': loc_information[4]}\n",
    "\n",
    "        # Look for the employer's response to the review, if it exists\n",
    "        if temp_dct['employerResponses'] != []:\n",
    "            temp_dct['employerResponses'] = re.findall('{\"response\":.*?,\"countHelpful\":', review_second_half)[0][:-16]+'}'\n",
    "\n",
    "        review_dct_lst.append(temp_dct)\n",
    "        \n",
    "    return review_dct_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'individual_reviews' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-c978efb6d655>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Empty the dictionaries\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mif\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindividual_reviews\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mindividual_reviews\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moverall_stats\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0moverall_stats\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'individual_reviews' is not defined"
     ]
    }
   ],
   "source": [
    "# Empty the dictionaries\n",
    "# 에러 떠도 무시하세요\n",
    "if bool(individual_reviews):\n",
    "    individual_reviews.clear()\n",
    "if bool(overall_stats):\n",
    "    overall_stats.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.4 company \"IBM\" reviews all scraped\n",
      "15 more companies to go\n",
      "---------------------------\n",
      "Next up: No.5 company\n"
     ]
    }
   ],
   "source": [
    "for name in names[:]:\n",
    "    \n",
    "    # Generate the company-specific part of the url\n",
    "    company_token = get_company_token(name)\n",
    "\n",
    "    # Access the Glassdoor reviews page\n",
    "    url = 'https://www.glassdoor.com/Reviews/' + company_token\n",
    "    page_number = ''\n",
    "    num = 2\n",
    "    html = '.htm'\n",
    "    page = requests.get(url+page_number+html, headers={'user-agent': 'Mozilla/5.0'})\n",
    "    webpage = page.text\n",
    "    apollo = webpage.split('<script>window.__APOLLO_STATE__')[1]\n",
    "\n",
    "    # First, get the overall stats\n",
    "    overall_stats = {name: get_overall_stats(apollo)}\n",
    "    \n",
    "    # Save the overall stats data into a text file\n",
    "    with open(re.sub(' ', '_', name)+'_overall_stats.txt', 'w') as file:\n",
    "         file.write(json.dumps(overall_stats))\n",
    "\n",
    "    # Secondly, get the individual reviews\n",
    "    individual_reviews = {dct['reviewId']: dct for dct in get_individual_reviews(apollo)} \n",
    "    end_of_reviews = False\n",
    "\n",
    "    while end_of_reviews is False:\n",
    "        page_number = '_P' + str(num) # Go to the next review page\n",
    "        page = requests.get(url+page_number+html, headers={'user-agent': 'Mozilla/5.0'})\n",
    "        webpage = page.text\n",
    "\n",
    "        if 'reviewId' not in webpage: # If we run out of pages, exit the while loop\n",
    "            end_of_reviews = True\n",
    "        else:\n",
    "            apollo = webpage.split('<script>window.__APOLLO_STATE__')[1]\n",
    "            for dct in get_individual_reviews(apollo):\n",
    "                individual_reviews[dct['reviewId']] = dct\n",
    "            \n",
    "            if num%300 == 0: # If we reach page 300, save the crawled data into a txt file and empty it\n",
    "                with open(re.sub(' ', '_', name)+'_individual_reviews_'+str(num/300)+'.txt', 'w') as file:\n",
    "                    file.write(json.dumps(individual_reviews))\n",
    "                individual_reviews.clear()\n",
    "                individual_reviews = {dct['reviewId']: dct for dct in get_individual_reviews(apollo)} \n",
    "            \n",
    "            num += 1\n",
    "            \n",
    "        time.sleep(randint(2,3))\n",
    "            \n",
    "    # Print if successfully scraped all the reviews\n",
    "    print('No.' + str(list(itconsulting_name).index(name)) + ' company \"' + name + '\" reviews all scraped')\n",
    "    \n",
    "    # Save the rest of the individual reviews data into a text file\n",
    "    with open(re.sub(' ', '_', name)+'_individual_reviews_end.txt', 'w') as file:\n",
    "         file.write(json.dumps(individual_reviews))\n",
    "            \n",
    "    # Empty the dictionarys\n",
    "    if bool(individual_reviews):\n",
    "        individual_reviews.clear()\n",
    "    if bool(overall_stats):\n",
    "        overall_stats.clear()\n",
    "\n",
    "    print(str(len(list(names)) - list(names).index(name) - 1) + ' more companies to go')\n",
    "    print('---------------------------')\n",
    "    time.sleep(5)\n",
    "    \n",
    "print('Next up: No.' + str(list(names).index(name) + 1) + ' company')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
