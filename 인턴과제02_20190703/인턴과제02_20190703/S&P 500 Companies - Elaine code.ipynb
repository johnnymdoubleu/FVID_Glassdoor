{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Persisting Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 리뷰 수가 10k+인 회사들의 리뷰가 제대로 크롤링되지 않음\n",
    "e.g.) Amazon (리뷰 30k)\n",
    "#### Glassdoor 자체적으로 데이터 수가 조금씩 어긋남 (일단 무시하기로 함)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working Station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the names of the S&P 500 companies\n",
    "snp500_alldat = pd.read_csv('SnP500.csv', delimiter=',')\n",
    "snp500_name = snp500_alldat.Security"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_company_token(company):\n",
    "    '''\n",
    "    Every company's Glassdoor review page url includes a randomly-assigned token (e.g., Booz Allen Hamilton: E2735)\n",
    "    This is an automated selenium program that collects the token for each of the S&P 500 companies\n",
    "    '''\n",
    "    \n",
    "    # Set up the webdriver (make sure that the driver is saved in the same path as the Jupyter notebook file)\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--ignore-certificate-errors')\n",
    "    options.add_argument(\"--test-type\")\n",
    "    options.binary_location = \"/usr/bin/chromium\"\n",
    "    driver = webdriver.Chrome() # paranthesis is empty since the driver is in the same path\n",
    "    driver.get('https://www.google.com') # it is faster to google than to search for the company on Glassdoor (crappy site)\n",
    "    \n",
    "    # Iterate through the list of S&P 500 company names\n",
    "    search_text = 'glassdoor ' + company + ' Reviews'\n",
    "    search = driver.find_element_by_name('q')\n",
    "    search.send_keys(search_text)\n",
    "    search.send_keys(Keys.RETURN) # hits return after you enter the search text\n",
    "    time.sleep(1)\n",
    "    links = driver.find_elements_by_partial_link_text('https://www.glassdoor.com/Reviews/')\n",
    "    full_link = links[0].get_attribute('href')\n",
    "    company_token = re.search('(?<=Reviews/)([\\w-]*)(?=.htm)',full_link)[1] # the token is between 'Reviews/' and '.htm'\n",
    "    driver.quit()\n",
    "    \n",
    "    return company_token\n",
    "\n",
    "\n",
    "def get_overall_stats(webpage_text):\n",
    "    '''\n",
    "    Overall summary of all the reviews posted for the company as of (today)\n",
    "    '''\n",
    "    overall_str = re.search('{\"overallRating\":[^;]*\"EmployerRatings\"}', webpage_text).group(0)\n",
    "    overall_dict = json.loads(overall_str)\n",
    "    \n",
    "    return overall_dict\n",
    "\n",
    "\n",
    "def get_individual_reviews(webpage_text):\n",
    "    '''\n",
    "    Each review posted for the company as of (today)\n",
    "    '''\n",
    "    review_lst = re.findall('{\"reviewId\":.*?(?:.reviews.\\d\":|\"ROOT_QUERY\")', apollo) # .*? means \"any\" ; ?: will prevent interference\n",
    "    review_lst = [review[:-12]+'}' for review in review_lst] # HARDCODING. need a better solution..\n",
    "    review_dct_lst = []\n",
    "\n",
    "    for review in review_lst:\n",
    "        review_first_half = review.split(',\"links\":')[0]+'}'\n",
    "        review_second_half = review.split(',\"links\":')[1]\n",
    "\n",
    "        try:\n",
    "            temp_dct = json.loads(review_first_half)\n",
    "        except ValueError:  # Includes simplejson.decoder.JSONDecodeError. Avoids JSONDecode Error: Invalid \\escape\n",
    "            if '\\\\<' in review:\n",
    "                temp_dct = json.loads(review_first_half.replace('\\\\<', '<')) # HARDCODING. need a better solution..\n",
    "            elif '\\\\>' in review:\n",
    "                temp_dct = json.loads(review_first_half.replace('\\\\>', '>')) # HARDCODING. need a better solution..\n",
    "\n",
    "        # Look for the reviewer's job title\n",
    "        reviewer_job = re.findall('(?<=,\"text\":\")(.*?)(?=\",\"__typename\":\"JobTitle\")', review_second_half)\n",
    "        try:\n",
    "            temp_dct['jobTitle'] = reviewer_job[0]\n",
    "        except:\n",
    "            temp_dct['jobTitle'] = reviewer_job\n",
    "\n",
    "        # Look for the reviewer's location\n",
    "        reviewer_loc = re.findall('(?<=,\"type\":\")(.*?)(?=\",\"__typename\":\"Location)', review_second_half)\n",
    "        if reviewer_loc == []:\n",
    "            temp_dct['Location'] = []\n",
    "        else:\n",
    "            loc_information = reviewer_loc[0].split('\"')\n",
    "            temp_dct['Location'] = {'locationType': loc_information[0], 'locationName': loc_information[4]}\n",
    "\n",
    "        # Look for the employer's response to the review, if it exists\n",
    "        if temp_dct['employerResponses'] != []:\n",
    "            temp_dct['employerResponses'] = re.findall('{\"response\":.*?,\"countHelpful\":', review_second_half)[0][:-16]+'}'\n",
    "\n",
    "        review_dct_lst.append(temp_dct)\n",
    "        \n",
    "    return review_dct_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty the dictionaries\n",
    "if bool(individual_reviews):\n",
    "    individual_reviews.clear()\n",
    "if bool(overall_stats):\n",
    "    overall_stats.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs fine except for the \"big companies\"\n",
    "\n",
    "for name in snp500_name[0:3]: # S&P 리스트 중 돌리고 싶은 회사를 여기서 설정할 수 있음\n",
    "    \n",
    "    # Generate the company-specific part of the url\n",
    "    company_token = get_company_token(name)\n",
    "\n",
    "    # Access the Glassdoor reviews page\n",
    "    url = 'https://www.glassdoor.com/Reviews/' + company_token\n",
    "    page_number = ''\n",
    "    num = 2\n",
    "    html = '.htm'\n",
    "    page = requests.get(url+page_number+html, headers={'user-agent': 'Mozilla/5.0'})\n",
    "    webpage = page.text\n",
    "    apollo = webpage.split('<script>window.__APOLLO_STATE__')[1]\n",
    "\n",
    "    # First, get the overall stats\n",
    "    overall_stats = {name: get_overall_stats(apollo)}\n",
    "    \n",
    "    # Save the overall stats data into a text file\n",
    "    with open(re.sub(' ', '_', name)+'_overall_stats.txt', 'w') as file:\n",
    "         file.write(json.dumps(overall_stats))\n",
    "\n",
    "    # Secondly, get the individual reviews\n",
    "    individual_reviews = {dct['reviewId']: dct for dct in get_individual_reviews(apollo)} \n",
    "    end_of_reviews = False\n",
    "\n",
    "    while end_of_reviews is False:\n",
    "        page_number = '_P' + str(num) # Go to the next review page\n",
    "        page = requests.get(url+page_number+html, headers={'user-agent': 'Mozilla/5.0'})\n",
    "        webpage = page.text\n",
    "\n",
    "        if 'reviewId' not in webpage: # If we run out of pages, exit the while loop\n",
    "            end_of_reviews = True\n",
    "        else:\n",
    "            apollo = webpage.split('<script>window.__APOLLO_STATE__')[1]\n",
    "            for dct in get_individual_reviews(apollo):\n",
    "                individual_reviews[dct['reviewId']] = dct\n",
    "            \n",
    "            if num%300 == 0: # If we reach page 300, save the crawled data into a txt file and empty it\n",
    "                with open(re.sub(' ', '_', name)+'_individual_reviews_'+str(num/300)+'.txt', 'w') as file:\n",
    "                    file.write(json.dumps(individual_reviews))\n",
    "                individual_reviews.clear()\n",
    "                individual_reviews = {dct['reviewId']: dct for dct in get_individual_reviews(apollo)} \n",
    "            \n",
    "            num += 1\n",
    "            \n",
    "        time.sleep(randint(2,3))\n",
    "            \n",
    "    # Print if successfully scraped all the reviews\n",
    "    print('No.' + str(list(snp500_name).index(name)) + ' company \"' + name + '\" reviews all scraped')\n",
    "    \n",
    "    # Save the rest of the individual reviews data into a text file\n",
    "    with open(re.sub(' ', '_', name)+'_individual_reviews.txt', 'w') as file:\n",
    "         file.write(json.dumps(individual_reviews))\n",
    "            \n",
    "    # Empty the dictionarys\n",
    "    if bool(individual_reviews):\n",
    "        individual_reviews.clear()\n",
    "    if bool(overall_stats):\n",
    "        overall_stats.clear()\n",
    "\n",
    "    print(str(len(list(snp500_name)) - list(snp500_name).index(name) - 1) + ' more companies to go')\n",
    "    print('---------------------------')\n",
    "    time.sleep(5)\n",
    "    \n",
    "print('Next up: No.' + str(list(snp500_name).index(name) + 1) + ' company')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
